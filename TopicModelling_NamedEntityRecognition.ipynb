{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MzbrwMki7Bj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy gensim spacy matplotlib pyLDAvis"
      ],
      "metadata": {
        "id": "cg00PoeQ_NRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf390a3-86b6-4c07-9291-0d3aed0d44a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim import corpora, models, similarities, utils\n",
        "from gensim.models import CoherenceModel, LdaMulticore\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "from multiprocessing import Pool\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
        "                        \n",
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/masterthesis/translated_reviews_sentiments.csv\")\n",
        "filter = ['ive', 'dont', 'got', 'youre', 'doesnt', 'game', 'dont', 'lets', 'thank', 'sea', 'mrs'] # manual filter after analyzing first results with only STOPWORDS OUT\n",
        "# Prepare data for topic modeling\n",
        "# Prepare data for topic modeling\n",
        "def prepare_data(data):\n",
        "    reviews = data[\"translated_comment\"].tolist()\n",
        "    \n",
        "    # Detect and merge bigrams using NLTK\n",
        "    tokenized_reviews = [nltk.word_tokenize(review.lower()) for review in reviews]\n",
        "    bigram_measures = BigramAssocMeasures()\n",
        "    finder = BigramCollocationFinder.from_documents(tokenized_reviews)\n",
        "    finder.apply_freq_filter(10)\n",
        "    bigram_phrases = finder.nbest(bigram_measures.pmi, 500)\n",
        "    bigram_transformer = Phrases(bigram_phrases)\n",
        "\n",
        "    # Add bigram phrases to the tokenizer\n",
        "    bigram_transformer = Phrases(bigram_phrases)\n",
        "    tokenized_reviews = [bigram_transformer[review] for review in tokenized_reviews]\n",
        "    \n",
        "    # Remove stop words and filter out rare and common words\n",
        "    tokenized_reviews = [[word for word in review if word not in STOPWORDS and word not in filter and len(word) > 2 and not word.isdigit()] for review in tokenized_reviews]\n",
        "    dictionary = Dictionary(tokenized_reviews)\n",
        "    dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
        "    corpus = [dictionary.doc2bow(text) for text in tokenized_reviews]\n",
        "    \n",
        "    return corpus, dictionary, tokenized_reviews\n",
        "\n",
        "\n",
        "corpus, dictionary, tokenized_reviews = prepare_data(data)\n",
        "\n",
        "# Perform LDA topic modeling\n",
        "def topic_modeling_lda(corpus, dictionary, num_topics):\n",
        "    lda_model = LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, iterations=100, passes=10, workers=2)\n",
        "    \n",
        "    return lda_model\n",
        "\n",
        "num_topics = 5\n",
        "lda_model = topic_modeling_lda(corpus, dictionary, num_topics)\n",
        "\n",
        "# Print topics\n",
        "for i, topic in lda_model.print_topics(-1):\n",
        "    print(f\"Topic {i + 1}: {topic}\")\n",
        "\n",
        "# Perform NER\n",
        "def named_entity_recognition(texts):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    named_entities = []\n",
        "\n",
        "    for text in texts:\n",
        "        doc = nlp(text)\n",
        "        for ent in doc.ents:\n",
        "            named_entities.append((ent.text, ent.label_))\n",
        "            \n",
        "    return named_entities\n",
        "\n",
        "named_entities = named_entity_recognition(data[\"translated_comment\"])\n",
        "\n",
        "# Analyze data\n",
        "def analyze_data(data, lda_model, corpus, named_entities):\n",
        "    # Add topics to the dataset\n",
        "    data[\"topic\"] = [sorted(lda_model.get_document_topics(corpus[i]), key=lambda x: -x[1])[0][0] for i in range(len(corpus))]\n",
        "    \n",
        "    # Add named entities to the dataset\n",
        "    data[\"named_entities\"] = ', '.join([f\"{ent[0]} ({ent[1]})\" for ent in named_entities])\n",
        "\n",
        "    # Group by sentiment_label and topic to identify common concerns and feedback\n",
        "    grouped_data = data.groupby([\"sentiment_label\", \"topic\"]).size().reset_index(name=\"count\")\n",
        "\n",
        "    return grouped_data\n",
        "\n",
        "grouped_data = analyze_data(data, lda_model, corpus, named_entities)\n",
        "print(grouped_data)\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "data.to_csv(\"translatedreviews_sentiments_with_topics.csv\", index=False)\n",
        "\n",
        "# Calculate coherence scores for different number of topics\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        try:\n",
        "            model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10, iterations=150, alpha='auto')\n",
        "            model_list.append(model)\n",
        "            coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "            coherence_values.append(coherencemodel.get_coherence())\n",
        "            print(\"+1 it√©ration\")\n",
        "        except ValueError:\n",
        "            print(f\"Skipping num_topics={num_topics} due to empty vocabulary\")\n",
        "            continue\n",
        "\n",
        "    return model_list, coherence_values\n",
        "    \n",
        "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=tokenized_reviews, start=2, limit=40, step=6)\n",
        "\n",
        "# Plot the coherence scores\n",
        "plt.plot(range(2, 40, 6), coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Train the LDA model with the best number of topics\n",
        "best_num_topics = np.argmax(coherence_values) * 6 + 2\n",
        "best_lda_model = model_list[np.argmax(coherence_values)]\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = gensimvis.prepare(best_lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(vis)\n",
        "\n",
        "# Save the LDA model visualization to a file\n",
        "pyLDAvis.save_html(vis, 'lda_model_visualization.html')\n",
        "\n",
        "# Combine the sentiment analysis with the topic modeling\n",
        "topic_sentiments = []\n",
        "for comment in data['translated_comment']:\n",
        "    bow = dictionary.doc2bow(gensim.utils.simple_preprocess(comment))\n",
        "    topic_probs = best_lda_model.get_document_topics(bow)\n",
        "    topic_sentiments.append(max(topic_probs, key=lambda x: x[1])[0])\n",
        "\n",
        "data['topic'] = topic_sentiments\n",
        "data['topic_label'] = data['topic'].apply(lambda x: 'Topic ' + str(x + 1))\n",
        "\n",
        "print(data[['translated_comment', 'sentiment_label', 'topic_label']])\n",
        "\n",
        "# Filter comments with negative sentiment and linked to privacy concerns\n",
        "def is_privacy_concern(comment):\n",
        "    privacy_keywords = ['privacy', 'personal data', \"tracing\", \"bluetooth\", \"ROBERT\", \"GDPR\"]\n",
        "    for keyword in privacy_keywords:\n",
        "        if keyword in comment.lower():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "data_privacy_neg = data[(data['sentiment_label'] == 'Negative') & (data['translated_comment'].apply(is_privacy_concern))]\n",
        "\n",
        "# Display the most relevant comments\n",
        "print(data_privacy_neg[['translated_comment', 'sentiment_label', 'topic_label']])\n",
        "\n",
        "# Save the most relevant comments to a CSV file\n",
        "data_privacy_neg.to_csv('relevant_comments.csv', index=False)\n",
        "\n",
        "# Visualize the distribution of topics for relevant comments\n",
        "topic_counts = data_privacy_neg['topic_label'].value_counts()\n",
        "plt.bar(topic_counts.index, topic_counts.values)\n",
        "plt.xlabel('Topics')\n",
        "plt.ylabel('Number of Comments')\n",
        "plt.title('Topic Distribution for Relevant Comments')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Visualize the distribution of sentiment scores for relevant comments\n",
        "sentiment_counts = data_privacy_neg['sentiment_label'].value_counts()\n",
        "plt.bar(sentiment_counts.index, sentiment_counts.values)\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Number of Comments')\n",
        "plt.title('Sentiment Distribution for Relevant Comments')\n",
        "plt.show()\n",
        "\n",
        "# Analyze the distribution of scores for relevant comments\n",
        "score_counts = data_privacy_neg['score'].value_counts().sort_index()\n",
        "plt.bar(score_counts.index, score_counts.values)\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Number of Comments')\n",
        "plt.title('Score Distribution for Relevant Comments')\n",
        "plt.xticks(range(1, 6))\n",
        "plt.show()\n",
        "\n",
        "# Function to display the most common words for a specific topic\n",
        "def display_common_words(topic_num, num_words=10):\n",
        "    topic_terms = best_lda_model.show_topic(topic_num, topn=num_words)\n",
        "    words = [term[0] for term in topic_terms]\n",
        "    print(f\"Common words for Topic {topic_num + 1}: {', '.join(words)}\")\n",
        "\n",
        "# Display the most common words for each topic\n",
        "for i in range(best_num_topics):\n",
        "    display_common_words(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dk3tNEALCZJc",
        "outputId": "686c9f4d-396f-4bce-f603-123a81964095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "/usr/local/lib/python3.9/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/pyLDAvis/_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  default_term_info = default_term_info.sort_values(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(coherence_values)\n",
        "print(model_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2afX_MpBaHaG",
        "outputId": "85b6a58e-8da0-46c3-9212-3c8af1cf393e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.40390643986481023, 0.48777124939923544, 0.43436257402910694, 0.4034073300246585, 0.42582599004540145, 0.42035250634029087, 0.40438324235953366]\n",
            "[<gensim.models.ldamodel.LdaModel object at 0x7f52a45d25b0>, <gensim.models.ldamodel.LdaModel object at 0x7f528833f6a0>, <gensim.models.ldamodel.LdaModel object at 0x7f52a5da06d0>, <gensim.models.ldamodel.LdaModel object at 0x7f52a08855e0>, <gensim.models.ldamodel.LdaModel object at 0x7f52a15edaf0>, <gensim.models.ldamodel.LdaModel object at 0x7f528f6031c0>, <gensim.models.ldamodel.LdaModel object at 0x7f52a5daa190>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the path to the directory in your Google Drive where you want to upload the files\n",
        "drive_path = '/content/drive/MyDrive/masterthesis/Topic_modeling_analysis/'\n",
        "\n",
        "# Upload the files to your Google Drive\n",
        "!cp /content/lda_model_visualization.html \"{drive_path}\"\n",
        "!cp /content/relevant_comments.csv \"{drive_path}\"\n",
        "#!cp /content/translatedreviews_sentiments_with_topics.csv \"{drive_path}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKC_wg-SpI5l",
        "outputId": "ede023a9-2b64-44aa-ccef-1709d7589228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the relevant_comments.csv file\n",
        "df = pd.read_csv('/content/drive/MyDrive/masterthesis/Topic_modeling_analysis/relevant_comments.csv')\n",
        "\n",
        "# Select only the columns we want\n",
        "df_new = df[['topic', 'preprocessed_comment', 'translated_comment']]\n",
        "\n",
        "# Save the new file to disk\n",
        "df_new.to_csv('new_relevant_comments.csv', index=False)"
      ],
      "metadata": {
        "id": "9xw03J8-zG3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the new CSV file with selected columns\n",
        "df = pd.read_csv('/content/new_relevant_comments.csv', usecols=['topic', 'preprocessed_comment'])\n",
        "\n",
        "# filter the dataframe to only include rows with topics 1, 3, or 7\n",
        "filtered_df = df[df['topic'].isin([1, 3, 7])]\n",
        "\n",
        "# select only the 'preprocessed_comment' column\n",
        "comments = filtered_df['preprocessed_comment']\n",
        "\n",
        "# write the comments to a text file\n",
        "with open('comments.txt', 'w') as f:\n",
        "    for comment in comments:\n",
        "        f.write(comment + '\\n')"
      ],
      "metadata": {
        "id": "xV3y60e1Ag5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STOP HERE FOR MIDTERM"
      ],
      "metadata": {
        "id": "ePAaBrZx7M_0"
      }
    }
  ]
}
